import logging
# Import the OllamaClient to interact with the Large Language Model (LLM).
from src.ollama_client import OllamaClient
import sys

# Configure logging for the module.
# Log messages will be displayed in the console (sys.stdout) with a timestamp,
# log level, and the message itself. INFO level messages and above will be shown.
logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='%(asctime)s - %(levelname)s - %(message)s')

class TeacherAgent:
    """
    The TeacherAgent is responsible for generating learning materials, evaluating student responses,
    and synthesizing new topics. It leverages an LLM via Ollama to perform these intelligent tasks.
    """
    def __init__(self, ollama_client: OllamaClient, teacher_model: str = None):
        """
        Initializes the TeacherAgent.

        Args:
            ollama_client (OllamaClient): An instance of OllamaClient to communicate with the LLM.
            teacher_model (str, optional): The specific LLM model to be used by the teacher.
                                           If None, the model configured in the ollama_client will be used.
        """
        self.ollama_client = ollama_client
        # If a specific teacher_model is provided, use it; otherwise, default to the client's model.
        self.teacher_model = teacher_model if teacher_model else ollama_client.model
        logging.info(f"TeacherAgent initialized with model: {self.teacher_model}")

    def generate_learning_material(self, topic: str, difficulty: str = "medium") -> str:
        """
        Generates learning material (e.g., a problem, a question, a scenario) for the student agent.
        The material is generated based on a specified topic and difficulty level, using the LLM.

        Args:
            topic (str): The subject or concept for which to generate learning material.
            difficulty (str, optional): The desired difficulty level of the material (e.g., "easy", "medium", "hard").
                                        Defaults to "medium".

        Returns:
            str: The generated learning material (e.g., a problem statement) or an error message.
        """
        # Construct a clear and specific prompt for the LLM to act as an expert teacher.
        prompt = (f"As an expert teacher, create a {difficulty} difficulty learning problem "
                  f"on the topic of '{topic}'. The problem should be concise and require "
                  f"the student to demonstrate understanding. Focus on a single concept.")
        
        logging.info(f"Teacher is thinking: 'I need a {difficulty} problem on {topic} for the student.'")
        
        # Send the prompt to the LLM via the Ollama client to get a generated response.
        response = self.ollama_client.generate_response(prompt, model=self.teacher_model)
        
        if response:
            logging.info(f"Teacher generated a problem: '{response[:100]}...'")
            return response
        else:
            logging.error(f"Teacher failed to generate learning material for topic '{topic}'.")
            return "Error: Could not generate learning material."

    def evaluate_student_response(self, problem: str, student_response: str) -> str:
        """
        Evaluates a student's response to a given problem and provides constructive feedback.
        The evaluation and feedback are generated by the LLM acting as an expert teacher.

        Args:
            problem (str): The original problem posed to the student.
            student_response (str): The student's answer or solution to the problem.

        Returns:
            str: Detailed feedback from the teacher (LLM), indicating correctness, strengths,
                 and areas for improvement, or an error message.
        """
        # Construct a prompt that asks the LLM to evaluate the student's response against the problem.
        prompt = (f"You are an expert teacher. Evaluate the following student response to the problem below.\n\n"
                  f"Problem: {problem}\n\n"
                  f"Student Response: {student_response}\n\n"
                  f"Provide concise feedback, pointing out strengths and areas for improvement. "
                  f"Also, indicate if the response is correct or incorrect.")
        
        logging.info(f"Teacher is evaluating student's response: '{student_response[:50]}...' to problem: '{problem[:50]}...'")

        # Send the evaluation prompt to the LLM.
        response = self.ollama_client.generate_response(prompt, model=self.teacher_model)

        if response:
            logging.info(f"Teacher provides feedback: '{response[:100]}...'")
            return response
        else:
            logging.error(f"Teacher failed to evaluate student response to problem: '{problem[:50]}...'.")
            return "Error: Could not evaluate student response."

    def synthesize_new_topic(self, current_topic: str, student_performance_summary: str = "") -> str:
        """
        Synthesizes a new learning topic that is a logical progression or addresses gaps
        based on the current topic and a summary of the student's performance.
        This allows the teacher to adapt its curriculum.

        Args:
            current_topic (str): The topic currently being taught or that was just completed.
            student_performance_summary (str, optional): A summary of how the student performed
                                                         on recent tasks, which can guide topic selection.

        Returns:
            str: A new, concisely proposed learning topic, or an error message.
        """
        # Prompt the LLM to act as an educator and suggest a new topic.
        prompt = (f"As an expert educator, you are teaching a student about '{current_topic}'.\n"
                  f"Based on the student's overall performance '{student_performance_summary}', "
                  f"propose a new, related learning topic that would be a logical next step or "
                  f"address a knowledge gap. Provide only the topic name, concisely.")
        
        logging.info(f"Teacher is considering '{current_topic}' and student performance to suggest a new topic.")

        # Get the new topic from the LLM.
        response = self.ollama_client.generate_response(prompt, model=self.teacher_model)

        if response:
            # Clean up the response (e.g., remove leading/trailing whitespace).
            new_topic = response.strip()
            logging.info(f"Teacher synthesized a new topic: '{new_topic}'.")
            return new_topic
        else:
            logging.error(f"Teacher failed to synthesize a new topic based on '{current_topic}'.")
            return "Error: Could not synthesize new topic."

    def evolve(self, performance_metric: float) -> None:
        """
        Initiates the evolution process for the TeacherAgent.
        This method allows the teacher's strategy to adapt over time based on overall
        student performance (or other metrics).

        Args:
            performance_metric (float): A quantitative measure of recent student performance
                                        (e.g., average reward).
        """
        logging.info(f"Teacher is evolving its teaching strategy based on student performance (metric: {performance_metric:.2f}).")
        # Example: if performance is high, perhaps increase difficulty or complexity of topics
        # if performance_metric > 0.7:
        #     self.difficulty_strategy = "adaptive_harder" # Teacher starts posing harder problems
        # else:
        #     self.difficulty_strategy = "adaptive_easier" # Teacher might pose easier problems
        pass

# Example usage block:
# This code runs only when the script is executed directly, not when imported as a module.
if __name__ == "__main__":
    # Import necessary modules for the example.
    from src.ollama_client import OllamaClient
    from src.config import OLLAMA_MODEL

    # Initialize the Ollama client instance using the configured model.
    ollama_client_instance = OllamaClient(model=OLLAMA_MODEL)

    # Proceed only if the Ollama server is running and accessible.
    if ollama_client_instance.is_ollama_running():
        # Instantiate the TeacherAgent.
        teacher = TeacherAgent(ollama_client=ollama_client_instance, teacher_model=OLLAMA_MODEL)

        # --- Test: Generating Learning Material ---
        print("\n--- Generating Learning Material ---")
        material = teacher.generate_learning_material("Reinforcement Learning", "easy")
        print(f"Generated Material:\n{material}")

        # --- Test: Evaluating Student Response (simulated) ---
        print("\n--- Evaluating Student Response ---")
        problem_example = "Explain the concept of exploration-exploitation dilemma in Reinforcement Learning."
        student_response_example = "The exploration-exploitation dilemma is about whether an agent should try new actions (explore) or stick to actions it knows are good (exploit)."
        feedback = teacher.evaluate_student_response(problem_example, student_response_example)
        print(f"Teacher Feedback:\n{feedback}")
        
        # --- Test: Synthesizing New Topic ---
        print("\n--- Synthesizing New Topic ---")
        new_topic = teacher.synthesize_new_topic("Basic Arithmetic", "Student consistently scores 100% on addition and subtraction.")
        print(f"Synthesized New Topic:\n{new_topic}")
        
        # --- Test: Evolution ---
        print("\n--- Testing Evolution ---")
        teacher.evolve(0.85) # Simulate good performance
        teacher.evolve(0.30) # Simulate poor performance
    else:
        # Inform the user if the Ollama server is not running.
        print("Ollama server is not running or model not available. Please ensure it's set up correctly.")